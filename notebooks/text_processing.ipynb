{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install spacy==3.5\n",
        "! python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-feX-dl3lTG8",
        "outputId": "53a6791a-b614-4f99-f69d-99808909c397"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==3.5\n",
            "  Downloading spacy-3.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (23.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (2.25.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (1.0.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (3.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (4.64.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (3.1.2)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (57.4.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (1.0.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (2.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (6.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (8.1.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (2.4.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (1.10.5)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (3.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (2.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.5) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy==3.5) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.5) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.5) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.5) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.5) (1.26.14)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy==3.5) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy==3.5) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy==3.5) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy==3.5) (2.1.2)\n",
            "Installing collected packages: spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.4\n",
            "    Uninstalling spacy-3.4.4:\n",
            "      Successfully uninstalled spacy-3.4.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed spacy-3.5.0\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-03-07 07:41:30.952749: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-07 07:41:32.787457: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-07 07:41:32.787612: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-07 07:41:32.787640: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-07 07:41:34.909173: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.5.0) (3.5.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.25.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.7)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 3.4.1\n",
            "    Uninstalling en-core-web-sm-3.4.1:\n",
            "      Successfully uninstalled en-core-web-sm-3.4.1\n",
            "Successfully installed en-core-web-sm-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy (https://spacy.io/) e NLTK (https://www.nltk.org/)"
      ],
      "metadata": {
        "id": "CnRtmMUWsyoN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0y2BL4zkLrZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e57ae19d-e353-4904-c9a2-a1fc1764f130"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfB_1LTNkPT7",
        "outputId": "c391339e-c63f-423c-fa7f-6fcba347bf8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-02-28 15:21:16.139673: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-28 15:21:16.139820: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-28 15:21:16.139841: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-02-28 15:21:18.499606: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.5.0                         \n",
            "Location         /usr/local/lib/python3.8/dist-packages/spacy\n",
            "Platform         Linux-5.10.147+-x86_64-with-glibc2.29\n",
            "Python version   3.8.10                        \n",
            "Pipelines        en_core_web_sm (3.5.0), pt_core_news_sm (3.5.0)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlFkaU2Akb5M"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O2iy_i9mcoW",
        "outputId": "5105e2d1-f4f5-4f2f-e161-646579ddf05d"
      },
      "source": [
        "#tokenization\n",
        "sentence = nlp(u'\"They\\'re leaving U.K. for U.S.A.\"')\n",
        "for word in sentence:\n",
        "  print(word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"\n",
            "They\n",
            "'re\n",
            "leaving\n",
            "U.K.\n",
            "for\n",
            "U.S.A.\n",
            "\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiVixa0Tmv0e",
        "outputId": "c36c6aea-66c6-44ba-8937-89155373f0d7"
      },
      "source": [
        "#tokenization\n",
        "sentence1 = nlp(u\"Hello, I am non-vegetarian, email me the menu at abc-xyz@gmai.com\")\n",
        "for word in sentence1:\n",
        "  print(word)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            ",\n",
            "I\n",
            "am\n",
            "non\n",
            "-\n",
            "vegetarian\n",
            ",\n",
            "email\n",
            "me\n",
            "the\n",
            "menu\n",
            "at\n",
            "abc-xyz@gmai.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence size\n",
        "len(sentence1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dlpto8TXBsT1",
        "outputId": "4c1222b5-aec2-410b-aeeb-b3336c3f3119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set([])\n",
        "for word in sentence1:\n",
        "  vocab.add(word.text)\n",
        "\n",
        "print(vocab)\n",
        "print(\"Size: \", len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wyh_imDRBroe",
        "outputId": "e7839741-8055-41b4-fcf5-1c7dbd096ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'-', 'Hello', 'I', 'email', 'abc-xyz@gmai.com', 'menu', 'at', 'vegetarian', 'am', 'the', 'non', 'me', ','}\n",
            "Tamanho:  13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Processing* long text"
      ],
      "metadata": {
        "id": "-d70sUOCCkw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "long_text = \"The son of a civil servant, Turing was educated at a top private school. \\\n",
        "He entered the University of Cambridge to study mathematics in 1931. After graduating \\\n",
        "in 1934, he was elected to a fellowship at King’s College (his college since 1931) in \\\n",
        "recognition of his research in probability theory. In 1936 Turing’s seminal paper \\\n",
        "“On Computable Numbers, with an Application to the Entscheidungsproblem [Decision Problem]” \\\n",
        "was recommended for publication by the American mathematical logician Alonzo Church, \\\n",
        "who had himself just published a paper that reached the same conclusion as Turing’s, although by a different method. Turing’s method (but not so much Church’s) had profound significance for the emerging science of computing. Later that year Turing moved to Princeton University to study for a Ph.D. in mathematical logic under Church’s direction (completed in 1938).\""
      ],
      "metadata": {
        "id": "V-jRrKbPCfss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "long_text_tokenized = nlp(long_text)\n",
        "print(\"# Tokens: \", len(long_text_tokenized))\n",
        "\n",
        "tokens = []\n",
        "for word in long_text_tokenized:\n",
        "  tokens.append(word.text)\n",
        "print(\"# Vocabulary size: \", len(set(tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqcUImziCptu",
        "outputId": "94a0007a-73c3-493c-a28b-6d90727e4a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Tokens:  161\n",
            "# Tamanho Vocabulario:  101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "tokens_freq = collections.Counter(tokens)"
      ],
      "metadata": {
        "id": "gKJ4KZkJDsjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_freq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrrFLwsZDxQM",
        "outputId": "a639927a-05a0-4621-d81e-b298b5b05511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'The': 1,\n",
              "         'son': 1,\n",
              "         'of': 4,\n",
              "         'a': 6,\n",
              "         'civil': 1,\n",
              "         'servant': 1,\n",
              "         ',': 5,\n",
              "         'Turing': 5,\n",
              "         'was': 3,\n",
              "         'educated': 1,\n",
              "         'at': 2,\n",
              "         'top': 1,\n",
              "         'private': 1,\n",
              "         'school': 1,\n",
              "         '.': 6,\n",
              "         'He': 1,\n",
              "         'entered': 1,\n",
              "         'the': 5,\n",
              "         'University': 2,\n",
              "         'Cambridge': 1,\n",
              "         'to': 5,\n",
              "         'study': 2,\n",
              "         'mathematics': 1,\n",
              "         'in': 6,\n",
              "         '1931': 2,\n",
              "         'After': 1,\n",
              "         'graduating': 1,\n",
              "         '1934': 1,\n",
              "         'he': 1,\n",
              "         'elected': 1,\n",
              "         'fellowship': 1,\n",
              "         'King': 1,\n",
              "         '’s': 6,\n",
              "         'College': 1,\n",
              "         '(': 3,\n",
              "         'his': 2,\n",
              "         'college': 1,\n",
              "         'since': 1,\n",
              "         ')': 3,\n",
              "         'recognition': 1,\n",
              "         'research': 1,\n",
              "         'probability': 1,\n",
              "         'theory': 1,\n",
              "         'In': 1,\n",
              "         '1936': 1,\n",
              "         'seminal': 1,\n",
              "         'paper': 2,\n",
              "         '“': 1,\n",
              "         'On': 1,\n",
              "         'Computable': 1,\n",
              "         'Numbers': 1,\n",
              "         'with': 1,\n",
              "         'an': 1,\n",
              "         'Application': 1,\n",
              "         'Entscheidungsproblem': 1,\n",
              "         '[': 1,\n",
              "         'Decision': 1,\n",
              "         'Problem': 1,\n",
              "         ']': 1,\n",
              "         '”': 1,\n",
              "         'recommended': 1,\n",
              "         'for': 3,\n",
              "         'publication': 1,\n",
              "         'by': 2,\n",
              "         'American': 1,\n",
              "         'mathematical': 2,\n",
              "         'logician': 1,\n",
              "         'Alonzo': 1,\n",
              "         'Church': 3,\n",
              "         'who': 1,\n",
              "         'had': 2,\n",
              "         'himself': 1,\n",
              "         'just': 1,\n",
              "         'published': 1,\n",
              "         'that': 2,\n",
              "         'reached': 1,\n",
              "         'same': 1,\n",
              "         'conclusion': 1,\n",
              "         'as': 1,\n",
              "         'although': 1,\n",
              "         'different': 1,\n",
              "         'method': 2,\n",
              "         'but': 1,\n",
              "         'not': 1,\n",
              "         'so': 1,\n",
              "         'much': 1,\n",
              "         'profound': 1,\n",
              "         'significance': 1,\n",
              "         'emerging': 1,\n",
              "         'science': 1,\n",
              "         'computing': 1,\n",
              "         'Later': 1,\n",
              "         'year': 1,\n",
              "         'moved': 1,\n",
              "         'Princeton': 1,\n",
              "         'Ph.D.': 1,\n",
              "         'logic': 1,\n",
              "         'under': 1,\n",
              "         'direction': 1,\n",
              "         'completed': 1,\n",
              "         '1938': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_tokens_freq = sorted(tokens_freq.items(), key = lambda x: x[1], reverse=True)\n",
        "sorted_tokens_freq[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uMbLkdGDZkF",
        "outputId": "85a931ca-780c-4526-814f-1cf300c8ed24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 6),\n",
              " ('.', 6),\n",
              " ('in', 6),\n",
              " ('’s', 6),\n",
              " (',', 5),\n",
              " ('Turing', 5),\n",
              " ('the', 5),\n",
              " ('to', 5),\n",
              " ('of', 4),\n",
              " ('was', 3),\n",
              " ('(', 3),\n",
              " (')', 3),\n",
              " ('for', 3),\n",
              " ('Church', 3),\n",
              " ('at', 2),\n",
              " ('University', 2),\n",
              " ('study', 2),\n",
              " ('1931', 2),\n",
              " ('his', 2),\n",
              " ('paper', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'A letter has been... written, asking him to be released! Have you seen it?'"
      ],
      "metadata": {
        "id": "I5AqWwZ_Es1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, word in enumerate(nlp(text)):\n",
        "  if word.is_punct==True:\n",
        "    print(\"{0}\\t{1}\".format(i, word.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_t3u7ysExRz",
        "outputId": "9d7f238e-cdee-4e6f-ae2d-26feeeff68cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\t...\n",
            "6\t,\n",
            "12\t!\n",
            "17\t?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nlp.Defaults.stop_words\n",
        "len(stopwords)"
      ],
      "metadata": {
        "id": "H6j3_6ngGRx4",
        "outputId": "52e0e442-778c-452b-d49a-6d5a0ae2b0a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "326"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords"
      ],
      "metadata": {
        "id": "eDVem6FUDEz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a32d34b8-2502-41d8-ba59-d6b63f58f90e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"'d\",\n",
              " \"'ll\",\n",
              " \"'m\",\n",
              " \"'re\",\n",
              " \"'s\",\n",
              " \"'ve\",\n",
              " 'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'across',\n",
              " 'after',\n",
              " 'afterwards',\n",
              " 'again',\n",
              " 'against',\n",
              " 'all',\n",
              " 'almost',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'already',\n",
              " 'also',\n",
              " 'although',\n",
              " 'always',\n",
              " 'am',\n",
              " 'among',\n",
              " 'amongst',\n",
              " 'amount',\n",
              " 'an',\n",
              " 'and',\n",
              " 'another',\n",
              " 'any',\n",
              " 'anyhow',\n",
              " 'anyone',\n",
              " 'anything',\n",
              " 'anyway',\n",
              " 'anywhere',\n",
              " 'are',\n",
              " 'around',\n",
              " 'as',\n",
              " 'at',\n",
              " 'back',\n",
              " 'be',\n",
              " 'became',\n",
              " 'because',\n",
              " 'become',\n",
              " 'becomes',\n",
              " 'becoming',\n",
              " 'been',\n",
              " 'before',\n",
              " 'beforehand',\n",
              " 'behind',\n",
              " 'being',\n",
              " 'below',\n",
              " 'beside',\n",
              " 'besides',\n",
              " 'between',\n",
              " 'beyond',\n",
              " 'both',\n",
              " 'bottom',\n",
              " 'but',\n",
              " 'by',\n",
              " 'ca',\n",
              " 'call',\n",
              " 'can',\n",
              " 'cannot',\n",
              " 'could',\n",
              " 'did',\n",
              " 'do',\n",
              " 'does',\n",
              " 'doing',\n",
              " 'done',\n",
              " 'down',\n",
              " 'due',\n",
              " 'during',\n",
              " 'each',\n",
              " 'eight',\n",
              " 'either',\n",
              " 'eleven',\n",
              " 'else',\n",
              " 'elsewhere',\n",
              " 'empty',\n",
              " 'enough',\n",
              " 'even',\n",
              " 'ever',\n",
              " 'every',\n",
              " 'everyone',\n",
              " 'everything',\n",
              " 'everywhere',\n",
              " 'except',\n",
              " 'few',\n",
              " 'fifteen',\n",
              " 'fifty',\n",
              " 'first',\n",
              " 'five',\n",
              " 'for',\n",
              " 'former',\n",
              " 'formerly',\n",
              " 'forty',\n",
              " 'four',\n",
              " 'from',\n",
              " 'front',\n",
              " 'full',\n",
              " 'further',\n",
              " 'get',\n",
              " 'give',\n",
              " 'go',\n",
              " 'had',\n",
              " 'has',\n",
              " 'have',\n",
              " 'he',\n",
              " 'hence',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hereafter',\n",
              " 'hereby',\n",
              " 'herein',\n",
              " 'hereupon',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'however',\n",
              " 'hundred',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'indeed',\n",
              " 'into',\n",
              " 'is',\n",
              " 'it',\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'keep',\n",
              " 'last',\n",
              " 'latter',\n",
              " 'latterly',\n",
              " 'least',\n",
              " 'less',\n",
              " 'made',\n",
              " 'make',\n",
              " 'many',\n",
              " 'may',\n",
              " 'me',\n",
              " 'meanwhile',\n",
              " 'might',\n",
              " 'mine',\n",
              " 'more',\n",
              " 'moreover',\n",
              " 'most',\n",
              " 'mostly',\n",
              " 'move',\n",
              " 'much',\n",
              " 'must',\n",
              " 'my',\n",
              " 'myself',\n",
              " \"n't\",\n",
              " 'name',\n",
              " 'namely',\n",
              " 'neither',\n",
              " 'never',\n",
              " 'nevertheless',\n",
              " 'next',\n",
              " 'nine',\n",
              " 'no',\n",
              " 'nobody',\n",
              " 'none',\n",
              " 'noone',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'nothing',\n",
              " 'now',\n",
              " 'nowhere',\n",
              " 'n‘t',\n",
              " 'n’t',\n",
              " 'of',\n",
              " 'off',\n",
              " 'often',\n",
              " 'on',\n",
              " 'once',\n",
              " 'one',\n",
              " 'only',\n",
              " 'onto',\n",
              " 'or',\n",
              " 'other',\n",
              " 'others',\n",
              " 'otherwise',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 'part',\n",
              " 'per',\n",
              " 'perhaps',\n",
              " 'please',\n",
              " 'put',\n",
              " 'quite',\n",
              " 'rather',\n",
              " 're',\n",
              " 'really',\n",
              " 'regarding',\n",
              " 'same',\n",
              " 'say',\n",
              " 'see',\n",
              " 'seem',\n",
              " 'seemed',\n",
              " 'seeming',\n",
              " 'seems',\n",
              " 'serious',\n",
              " 'several',\n",
              " 'she',\n",
              " 'should',\n",
              " 'show',\n",
              " 'side',\n",
              " 'since',\n",
              " 'six',\n",
              " 'sixty',\n",
              " 'so',\n",
              " 'some',\n",
              " 'somehow',\n",
              " 'someone',\n",
              " 'something',\n",
              " 'sometime',\n",
              " 'sometimes',\n",
              " 'somewhere',\n",
              " 'still',\n",
              " 'such',\n",
              " 'take',\n",
              " 'ten',\n",
              " 'than',\n",
              " 'that',\n",
              " 'the',\n",
              " 'their',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'thence',\n",
              " 'there',\n",
              " 'thereafter',\n",
              " 'thereby',\n",
              " 'therefore',\n",
              " 'therein',\n",
              " 'thereupon',\n",
              " 'these',\n",
              " 'they',\n",
              " 'third',\n",
              " 'this',\n",
              " 'those',\n",
              " 'though',\n",
              " 'three',\n",
              " 'through',\n",
              " 'throughout',\n",
              " 'thru',\n",
              " 'thus',\n",
              " 'to',\n",
              " 'together',\n",
              " 'too',\n",
              " 'top',\n",
              " 'toward',\n",
              " 'towards',\n",
              " 'twelve',\n",
              " 'twenty',\n",
              " 'two',\n",
              " 'under',\n",
              " 'unless',\n",
              " 'until',\n",
              " 'up',\n",
              " 'upon',\n",
              " 'us',\n",
              " 'used',\n",
              " 'using',\n",
              " 'various',\n",
              " 'very',\n",
              " 'via',\n",
              " 'was',\n",
              " 'we',\n",
              " 'well',\n",
              " 'were',\n",
              " 'what',\n",
              " 'whatever',\n",
              " 'when',\n",
              " 'whence',\n",
              " 'whenever',\n",
              " 'where',\n",
              " 'whereafter',\n",
              " 'whereas',\n",
              " 'whereby',\n",
              " 'wherein',\n",
              " 'whereupon',\n",
              " 'wherever',\n",
              " 'whether',\n",
              " 'which',\n",
              " 'while',\n",
              " 'whither',\n",
              " 'who',\n",
              " 'whoever',\n",
              " 'whole',\n",
              " 'whom',\n",
              " 'whose',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'within',\n",
              " 'without',\n",
              " 'would',\n",
              " 'yet',\n",
              " 'you',\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " '‘d',\n",
              " '‘ll',\n",
              " '‘m',\n",
              " '‘re',\n",
              " '‘s',\n",
              " '‘ve',\n",
              " '’d',\n",
              " '’ll',\n",
              " '’m',\n",
              " '’re',\n",
              " '’s',\n",
              " '’ve'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, word in enumerate(nlp(text)):\n",
        "  if word.is_stop==True:\n",
        "    print(\"{0}\\t{1}\".format(i, word.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8UM-OK5GBJT",
        "outputId": "6edcea1b-6d09-4081-89ab-fe11fddac7e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tA\n",
            "2\thas\n",
            "3\tbeen\n",
            "8\thim\n",
            "9\tto\n",
            "10\tbe\n",
            "13\tHave\n",
            "14\tyou\n",
            "16\tit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(nlp(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2jPIbrEDhjz",
        "outputId": "c9b9f568-0bac-4ddf-9b78-8cd6894b3027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IPcaWPlkczj",
        "outputId": "441cd96c-7690-428a-e8ed-04f09feaefc2"
      },
      "source": [
        "#sentence segmentation\n",
        "doc = nlp(\"I Love U.S.A. I've been living in N.Y.C. for more than 10 years now.\")\n",
        "#doc = nlp(\"This is a sentence. This is another sentence.\")\n",
        "\n",
        "assert doc.has_annotation(\"SENT_START\")\n",
        "\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I Love U.S.A.\n",
            "I've been living in N.Y.C. for more than 10 years now.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYtnzck6oFe8",
        "outputId": "838e8e4f-4d55-4e25-c55f-caa106e0cdb7"
      },
      "source": [
        "#lemmatization\n",
        "sentence2 = nlp(u'A letter has been written, asking him to be released')\n",
        "for word in sentence2:\n",
        "    print(word.text + '  ===>', word.lemma_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A  ===> a\n",
            "letter  ===> letter\n",
            "has  ===> have\n",
            "been  ===> be\n",
            "written  ===> write\n",
            ",  ===> ,\n",
            "asking  ===> ask\n",
            "him  ===> he\n",
            "to  ===> to\n",
            "be  ===> be\n",
            "released  ===> release\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBAWylU6rFjs",
        "outputId": "6d18d64d-cada-4ded-a19e-f298b1b623ac"
      },
      "source": [
        "#stemming\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "tokens = ['compute', 'computer', 'computed', 'computing']\n",
        "for token in tokens:\n",
        "    print(token + ' --> ' + stemmer.stem(token))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compute --> comput\n",
            "computer --> comput\n",
            "computed --> comput\n",
            "computing --> comput\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TTGg0cTrVOM",
        "outputId": "961cc0f2-f026-40f1-e9c2-c64e9b552538"
      },
      "source": [
        "#lemmatization\n",
        "sentence3 = nlp(u'compute computer computed computing')\n",
        "for word in sentence3:\n",
        "    print(word.text + '  ===>', word.lemma_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compute  ===> compute\n",
            "computer  ===> computer\n",
            "computed  ===> compute\n",
            "computing  ===> computing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lemmatization\n",
        "https://cst.dk/tools/index.php"
      ],
      "metadata": {
        "id": "YEoY32ododR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In Italian"
      ],
      "metadata": {
        "id": "wUMoZUotohlf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1l1eaqKvh5m",
        "outputId": "fedb4f5d-fb00-44d0-9252-3a49a4134f5a"
      },
      "source": [
        "!python -m spacy download it_core_news_sm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-03-07 07:41:54.460674: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-07 07:41:56.599665: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-07 07:41:56.599831: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-07 07:41:56.599859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-07 07:42:02.009127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting it-core-news-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.5.0/it_core_news_sm-3.5.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.8/dist-packages (from it-core-news-sm==3.5.0) (3.5.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (4.64.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (8.1.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2.25.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (1.10.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2.1.2)\n",
            "Installing collected packages: it-core-news-sm\n",
            "Successfully installed it-core-news-sm-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMd766uVu0ro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9bbf193-e004-421b-8b48-c93c5d534663"
      },
      "source": [
        "import spacy\n",
        "nlp1 = spacy.load('it_core_news_sm')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNkeqZWrvv-0",
        "outputId": "d2733bda-937b-441b-93f3-3306abd334c8"
      },
      "source": [
        "sentence4 = nlp1('Roma era la capitale dell\\'Impero Romano.')\n",
        "for word in sentence4:\n",
        "    print(word.text + '  ===>', word.lemma_)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Roma  ===> Roma\n",
            "era  ===> essere\n",
            "la  ===> il\n",
            "capitale  ===> capitale\n",
            "dell'  ===> di il\n",
            "Impero  ===> impero\n",
            "Romano  ===> Romano\n",
            ".  ===> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_it = nlp1.Defaults.stop_words"
      ],
      "metadata": {
        "id": "0wxM2sD447qO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords_it)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH22Pa-H4_1H",
        "outputId": "d36cc92d-a6e3-4c2a-b475-ef2135c56dd9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'avrebbe', 'lasciato', 'delle', 'agl', 'aver', 'facevamo', 'fosse', 'ecc', 'circa', 'noi', 'città', 'conciliarsi', 'farebbero', 'qualcuna', 'codesto', 'dovra', 'dovunque', 'senza', 'allo', 'avessimo', 'in', 'non', 'bene', 'trenta', 'ahimè', 'parecchie', 'hai', 'attraverso', 'stiate', 'una', 'certa', 'agli', 'cento', 'effettivamente', 'quattro', 'salvo', 'stettero', 'fin', 'possedere', 'fuori', 'sarei', 'fossero', 'quantunque', 'dov', 'nostra', 'ulteriore', 'sotto', 'ebbero', 'altrui', 'quelli', 'oppure', 'stati', 'srl', 'prima', 'chicchessia', 'gli', 'stette', 'miliardi', 'meglio', 'avuto', 'al', 'certi', 'tale', 'grande', 'negl', 'troppo', 'sembrare', 'quante', 'stessi', 'questi', 'anche', 'li', 'questa', 'nuovo', 'tua', 'fra', 'stessero', 'nella', 'dietro', 'abbiate', 'diventato', 'contro', 'starà', 'fummo', 'anno', 'altrimenti', 'tranne', 'alcuno', 'avevano', 'stanno', 'scopo', 'tutti', 'abbiano', 'vostra', 'niente', 'tu', 'siamo', 'fece', 'voi', 'sia', 'tutto', 'facevo', 'lungo', 'quasi', 'quanta', 'farai', 'dagl', 'di', 'vari', 'faccia', 'altro', 'nei', 'degl', 'riecco', 'haha', 'ora', 'fa', 'stessa', 'col', 'ebbi', 'pure', 'tue', 'vicino', 'facevi', 'nemmeno', 'degli', 'mie', 'quale', 'cio', 'volte', 'avevo', 'sull', 'proprio', 'avevi', 'steste', 'avesse', 'mancanza', 'facciate', 'gliele', 'posto', 'hanno', 'ossia', 'così', 'avendo', 'lei', 'inoltre', 'nazionale', 'avrò', 'stata', 'gliela', 'nessuno', 'quest', 'stessimo', 'sempre', 'registrazione', 'favore', \"d'\", 'alla', 'tutta', 'ieri', 'avessero', 'soltanto', 'facendo', 'certo', 'mio', 'bravo', 'eppure', 'tra', 'intorno', 'pochissimo', 'avute', 'è', 'solo', 'diventare', 'dagli', 'sua', 'tuoi', 'stava', 'deve', 'starete', 'relativo', 'anni', 'quali', 'abbastanza', 'facesse', 'varie', 'perché', 'vario', 'aveste', 'sul', 'stesse', 'altri', 'sugli', 'intanto', 'quel', 'avreste', 'saremmo', 'qualunque', 'concernente', \"un'\", 'avanti', 'medesimo', 'saresti', 'tanto', 'quella', 'certe', 'insieme', 'avete', 'mediante', 'improvviso', 'mentre', 'titolo', 'fecero', 'successivamente', 'miei', 'cosa', 'qui', 'sugl', 'esser', 'spesso', 'infatti', 'od', 'gia', 'allora', 'quindi', 'starò', 'detto', 'già', 'avuta', 'tuo', 'alcuni', 'esempio', 'ultimo', 'sareste', \"nient'\", 'parecchio', 'dappertutto', 'vostri', 'finalmente', 'mondo', 'avremo', 'talvolta', 'farebbe', 'diventa', 'press', 'esse', 'dai', 'mezzo', 'ogni', 'nessun', 'quello', 'faremo', 'nostre', 'codesta', 'ansa', 'giacche', 'cima', 'per', 'ella', 'ai', 'nell', 'avevamo', 'quando', 'durante', 'sembri', 'modo', 'seguente', 'però', 'facciano', 'avuti', 'la', 'recentemente', 'dalle', 'presa', 'sembrato', 'stesso', 'ho', 'secondo', 'nondimeno', 'starai', 'peggio', 'vi', 'dallo', 'peccato', 'poi', 'minimi', 'marche', 'faresti', 'avemmo', 'avremmo', 'lontano', 'vostro', 'piu', \"nessun'\", 'glielo', 'basta', 'tali', 'gliene', 'mai', 'essendo', 'qualche', \"t'\", 'starei', 'citta', 'giorni', 'scola', 'poiche', 'su', 'saranno', 'chi', 'ognuno', 'governo', 'vita', 'dovrà', 'stavate', 'aveva', 'parecchi', 'colei', 'uguali', 'adesso', 'feci', 'dell', 'entrambi', 'facevano', 'recente', 'affinche', 'grazie', 'quelle', 'io', 'dei', 'essere', 'trovato', 'po', 'che', 'avente', 'nostro', 'stareste', 'solito', 'ecco', \"c'\", \"l'\", 'avrà', 'principalmente', 'questo', 'mesi', 'cortesia', 'dall', 'malgrado', 'osi', 'all', 'scorso', 'accidenti', 'seguito', 'egli', 'perciò', 'ne', 'oltre', 'starebbe', 'dunque', 'il', 'ci', 'e', 'tutte', \"m'\", 'persone', 'purtroppo', 'verso', 'volta', 'visto', 'tre', 'successivo', 'faremmo', 'sto', 'coll', 'stando', 'attesa', 'ahime', 'ottanta', 'fino', 'nulla', 'fare', 'sarai', 'ad', 'eravate', 'invece', 'siano', 'nessuna', 'molto', 'piuttosto', 'dice', 'magari', 'co', 'sullo', 'facessi', 'nove', 'sarebbero', 'sono', 'ancora', \"dall'\", 'faceste', 'nonsia', 'moltissimo', 'posteriore', 'realmente', 'giorno', 'stemmo', 'macche', 'abbiamo', 'due', 'avesti', 'otto', 'piedi', 'ed', 'dirimpetto', 'perche', 'ebbe', 'partendo', 'te', 'facemmo', 'molti', 'dopo', 'stia', 'pero', 'momento', 'saro', 'starebbero', 'stetti', 'perfino', 'forse', 'lato', 'me', \"quel'\", 'consiglio', 'state', 'del', 'faceva', 'fanno', 'colui', 'stavamo', 'fatto', 'stavo', 'lui', 'saremo', 'avevate', 'cos', 'staresti', 'neppure', 'mila', 'fine', 'nelle', 'pieno', 'staremmo', 'caso', 'avresti', 'qualcosa', 'codesti', 'piglia', 'sig', 'dove', 'suoi', 'frattempo', 'farete', 'nel', 'mosto', 'facessero', 'loro', 'chiunque', 'staranno', 'fui', 'alcuna', 'sei', 'ero', 'ti', 'a', 'fareste', 'erano', 'può', 'sarà', 'là', 'sulla', 'avrei', 'furono', 'finche', 'sulle', 'luogo', 'essi', 'cosi', 'fai', 'davanti', 'brava', 'cui', 'mi', 'ministro', 'gruppo', 'stavano', 'casa', 'avrete', 'male', 'percio', 'dire', 'benissimo', 'dal', 'ha', 'stiamo', 'nello', 'sue', 'negli', 'quanto', 'fossi', 'fu', 'via', 'stato', 'come', 'se', 'futuro', 'con', \"dell'\", 'ex', 'vostre', 'paese', 'ciascuno', 'subito', 'coi', 'conclusione', 'glieli', 'sui', \"v'\", 'staremo', 'sta', 'preferibilmente', 'abbia', 'forza', 'vale', 'avessi', 'puo', 'sopra', 'varia', \"gl'\", 'da', 'eravamo', 'facessimo', \"s'\", 'altrove', 'possa', 'sara', 'meno', 'siate', 'avrai', 'fosti', 'oggi', 'era', 'si', 'malissimo', 'suo', 'farei', 'foste', 'cioe', 'coloro', 'lo', 'milioni', 'stesti', 'siete', 'sette', 'le', 'nostri', 'no', 'cominci', 'della', 'faranno', 'generale', 'più', 'alle', 'uno', 'facciamo', 'lavoro', 'avrebbero', 'novanta', 'anticipo', 'primo', 'va', 'ma', 'eri', 'avranno', 'dentro', 'persino', 'un', 'maggior', 'fossimo', 'potrebbe', 'stiano', 'quanti', 'nonostante', 'faccio', 'mia', 'avere', 'poco', 'inc', \"quest'\", 'ciascuna', 'sembra', 'stavi', 'qualcuno', 'tuttavia', 'cogli', 'probabilmente', 'ognuna', 'uomo', 'ore', 'stai', 'dello', 'sarete', 'queste', 'parte', 'assai', 'farò', 'tempo', 'facevate', 'farà', 'dalla', 'averlo', 'sarò', 'facesti', 'sarebbe', 'comunque'}\n"
          ]
        }
      ]
    }
  ]
}